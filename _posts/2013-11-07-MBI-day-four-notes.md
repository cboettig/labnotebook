---
layout: post
tags: 
- conference
- pdg_control
category: ecology

---


## Morning session

#### Iadine Chades

- Chad√®s, I., Carwardine, J., Martin, T.G., Nicol, S., Sabbadin, R. & Buffet, O. (2012) MOMDPs: a solution for modelling adaptive management problems. The Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI-12), pp. 267-273. Toronto, Canada.

- 10.1098/rspb.2013.0325 Migratory connectivity magnifies the consequences of habitat loss from sea-level rise for shorebird populations

#### Jake LaRiviera

presents the challenges of the uncertainty table.  Additional challenges in making an apples-to-apples comparison of the benefit of decreasing noise of different systems (e.g. in pricing information?)  

#### Me

Some good questions following talk, primarily on BNP part.  

- Where does the risk-adverse vs risk-prone behavior come from?  Adjusting curvature of the uncertainty appropriately.  
- Any lessons after stock collapsed, e.g. Rebuild a stock rather than mantain it? (Perhaps, but may face hysteresis in a way the intial collapse does not).  
- Brute-force passive learning?  

## Afternoon discussion


1. Is an active learning approach more or less valuable in a changing environment
2. Embracing surprise: how do we actually mathematically do this.
3. Limitations due to constraints on frequency of updating. (e.g. we don't get to change harvest, we get to set a TAC once every ten years).  
4. Uncertainty affecting net present value vs affecting model behavior



<!--
### Misc comments on L & L

Brian, great post here.  

To be honest, it sounds like L&L are primarily concerned about bad statistical practices rather than about open data, ecoinformatics, etc.  They speculate on a rising number of publications that make mistakes in multiple comparisons, in ignoring heterogeneity in data, in failing to have well-posed questions first.  This blog has often spoken to the issue of pervasiveness of statistical errors in the ecological literature, which is certainly neither a new problem.  As many commenters have pointed out, those ecologists not collecting their own data are perhaps not the leading culprits in making these statistical errors.  

I completely appreciate the deep knowledge of context that comes with first-hand collection of data, but at least to the extent that the context informs the conclusions, that context should be made part of the metadata.  Do we not move towards, not away, from enabling junk science when we permit conclusions to be drawn only by those who designed the experiment and collect the data?  

It is too easy to read L&L as a defense of data hugging.  You can't use my data because you cannot understand it.  If publications and citations didn't play the role they do in our academic economy, I suspect we would hear the exact same sentiments about publications themselves: if I publish my ideas, people who lack my superior understanding of context will misunderstand and misuse them!  When these contributions are rewarded, it is easier to appreciate the benefits having these ideas be seen and debated among the scientific community outweigh the risk of misintepretation.  The same might be said of data.  

Open data is often represented as benefiting the parasites (through "free" publications) at a cost to the original contributors (in "lost" publications).  If the production and citation of one's original datasets became the major metric of scientific contribution, this would reward all those collecting data while leaving the parasites without any recognized contribution - a valuation that sounds more in line with how L&L view the reliatve merits of these groups.  Given their position, it seems they would be great advocates for data publication and citation, which rewards the things they value most, and overhauling the current publication system so favorable to the existence of parasites.  


Phylogenetic data is perhaps the most pervasive example across ecological and evolutionary work where people now use sequences or phylogeny data they did not collect themselves.  And yes, the literature is rife with examples where this data has been misapplied - such as branch lengths being misinterpreted or uncertainty in branch lengths ignored.  Would fewer errors be made if we had no genbank and everyone had to collect all their own sequence data?  Yes, because most of these studies would become infeasible anyhow.  Creating a time-calibrated phylogeny from sequence data and fossils you collected all yourself might give you a greater appreciation for the uncertainty, but only to the extent that you learn the theoretical statistical underpinnings of the inference along the way.  

-->
