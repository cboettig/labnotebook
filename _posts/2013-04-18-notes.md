---
layout: post
category: ecology
tags: 
- pmc
- nonparametric-bayes
- warning-signals
- code-tricks 

---

## Misc coding

* knitcitations handling of html formatting in tooltip: html inside the tag must needs be escaped. [knitcitations/#37](https://github.com/cboettig/knitcitations/issues/37) 

* pmc functions

hmm.. how do we easily wrap a function such that it returns its arguments (given and default) in a named list (appropriate for `do.call(function_name, args_list)`)? `match.call()` inside the function will give the literal string of how the function was called, which, if evaluated in the same environment with same variables in working space, will return the same results, but doesn't lend itself to updating any one of those arguments.  Hoped to extract variable values with some combination of `formals` and `match.arg()` but no luck.  

* Mendeley API authentication solved using [Michal's clever implementation for OAuth](https://github.com/michalboo/mendeley-oapi/blob/master/authorise.rb), see my original query on [SO](http://stackoverflow.com/questions/15887366/obtaining-an-oauth-token-in-ruby-for-the-mendeley-api)

## nonparametric-bayes

* Implement the SDP calculation for the posterior distributions returned in the parametric comparison case [nonparametric-bayes/allen.Rmd](https://github.com/cboettig/nonparametric-bayes/blob/795bf96ed92a708b44978610da7db3b49ebb4dce/inst/examples/BUGS/allen.Rmd).  
* Working out the integration over uncertainty for the parameter distribution.  Brute force approach clearly doesn't scale, even on coarse grid, to many parameters.  After discussion with Marc, implementing as Monte Carlo instead.  As we get samples from the posterior out of the BUGs fit in the first place, this is much more efficient.  Should of tried that the first time.  [nonparametric-bayes/par_uncertainty_sdp.Rmd](https://github.com/cboettig/nonparametric-bayes/blob/34922e969e984b5d695a5b53aee01aac1ea3b38e/inst/examples/BUGS/par_uncertainty_sdp.md)


## warning-signals

* Analytic calculation of eigenvalues for managed vs unmanaged system
* finish fallacy reply
* finish ews-review reply


## Geospatial coordinates from plain text

Discussed the problem of geolocation from plain text discriptions with Simon a few days ago.  I've written a simple i[R function to query place names against the Google Map API](https://github.com/cboettig/sandbox/blob/29ab32342e9c07272b4796778cd86d518368b3b2/R-tricks/fishphotos.R), but as Simon has demonstrated, this will often return the coordinates of a cafe in San Fransisco rather than the lake in Italy you meant to get.  At DataCite 2011 I had met some folks who mentioned developing a machine learning algorithm for this, though stupidly it looks like I did not record this in my [notes](http://carlboettiger.info/2011/08/24/datacite-day-1.html).  Oh well, Google turned up the following interface:  [http://geoparser.digmap.eu/advanced.jsp](http://geoparser.digmap.eu/advanced.jsp)

Click the geoparse example and scroll down in the XML; they assign probabilities of possible spatial matches.  For instance, Hiroshima as the name of a "railroad station" gets 0.05 probability, whereas the majority of the weight falls on "populated place" in Japan.  

It looks like you could query against the service directly, haven't really explored.  

They have a paper out on this too, [http://oa.upm.es/4367/1/INVE_MEM_2008_60075.pdf](http://oa.upm.es/4367/1/INVE_MEM_2008_60075.pdf)


## Reading


* <span class="showtooltip" data-html="true" title="<p>Shelton A, Satterthwaite W, Beakes M, Munch S, Sogard S and Mangel M (2013). &ldquo;Separating Intrinsic And Environmental Contributions to Growth And Their Population Consequences.&rdquo; The American Naturalist, pp. 000&ndash;000. ISSN 00030147."><a href="http://dx.doi.org/10.1086/670198" rel="http://purl.org/spar/cito/discusses" >Shelton _et. al._ (2013)</a></span>  provide a rather elegant example of how accounting for individual variation improves modeling efforts in growth dynamics.  
*  Measurement error is important but difficult to deal with; a proper state-space model should use all the replicates to estimate the measurement error process.  <span class="showtooltip" data-html="true" title="<p>Knape J, Besbeas P and de Valpine P (2013). &ldquo;Using Uncertainty Estimates in Analyses of Population Time Series.&rdquo; Ecology, pp. 130417174952006&ndash;. ISSN 0012-9658."><a href="http://dx.doi.org/10.1890/12-0712.1" rel="http://purl.org/spar/cito/critiques" >Knape _et. al._ (2013)</a></span>  argue that you can do pretty well with just the standard errors from the replicates. -- Um, but don't we know this?  We expect the central limit theorem to be pretty good a lot of the time.  The demonstration that it can work is not quite as useful as a demonstration of when we might expect it not to; but my quick read may be missing something.  Certainly an interesting point to wrestle with before going off the deep end with the state-space modeling each time...

* Looking over an early draft of Duncan and Deborah's "XML and Web Technologies for Data Sciences with R". Looks fantastic -- where was this 3 years ago?

* Just launched: http://bison.usgs.ornl.gov (US only). Individual sample occurances, but no morphological measurements available.  

## References


- Jonas Knape, Panagiotis Besbeas, Perry de Valpine,   (2013) Using Uncertainty Estimates in Analyses of Population Time Series.  *Ecology*  130417174952006-NA  [10.1890/12-0712.1](http://dx.doi.org/10.1890/12-0712.1)
- Andrew O. Shelton, William H. Satterthwaite, Michael P. Beakes, Stephan B. Munch, Susan M. Sogard, Marc Mangel,   (2013) Separating Intrinsic And Environmental Contributions to Growth And Their Population Consequences.  *The American Naturalist*  000-000  [10.1086/670198](http://dx.doi.org/10.1086/670198)

